Ray-guided volume rendering is the way to do volume rendering: 5 TB
dataset in under a second!
	. we have shown how fast such a renderer can be
	. order of magnitude faster than other methods
	. speedup comes mostly from only loading data that are needed

Reorganizing the data continues to be the bane of high-performance
volume rendering.
	. just reading a terabyte on a 100mb/s disk takes 2.9125 hours
	. reorganization is lots of scattered writes and reads, can't perform well
	. at heart, data movement: this is only going to get worse
	. mitigation work is possible, but still no solution on the horizon

`fat' nodes are the way to go, at scale.
	. same techniques as desktop work in distributed mem
	. KD-tree decomposition and binary swap compositing works well enough
	. compositing is not an issue: don't use so many nodes.

%readback / pushing data to the GPU is really not an issue, as shown in
%both HPG2010 as well as LDAV2013.

% Mesa-based rendering is wicked slow and never worth it.

% load balancing *can* help, but in general is not a good idea at scale---yet.

current IO APIs are ill-suited to the task: there is no way to specify
information that the IO subsystem needs for efficient operation.
	. need to match IO of distributed file storage to node requesting/using it
	. current APIs force everything into stream interface
	. middleware helps, but this needs to interact with VFS, at the core.

In situ visualization is currently complex and difficult.
\begin{itemize}
	\item large APIs
	\item unstable
	\item lots of metadata to convey
	\item when to interrupt sim / balance of sim vs. vis time
\end{itemize}

We have shown that in situ visualization can be considerably more
simple than previously expected.
\begin{itemize}
	\item works with normal compiled versions
	\item majority of metadata is worthless
\end{itemize}

Soon enough, visualization \& analysis will \emph{only} be done during
simulation.
