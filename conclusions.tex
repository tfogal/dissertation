In this dissertation, we have:
\begin{itemize}
	\item demonstrated a forward-looking architecture for volume visualization,
	\item supported the approach of ray-guided rendering with extensive
benchmarks,
	\item established the multi-scale parallelism architecture that future
	visualization---and HPC---work is following,
	\item identified a number of best practices for the remaining largest
challenge in performance-oriented visualization: IO, and
	\item outlined methods to profoundly simplify the practice of \textit{in
situ} visualization.
\end{itemize}
Here we offer some concluding remarks and projections based on the work
developed here.

We began by outlining the volume rendering architecture,
``\textit{Tuvok}''.  The architecture strikes a balance between
performance, practicality, and portability that few have done.  Volume
rendering modules in Voreen~\cite{Voreen:2009} have since followed
a similar architecture.  However, academic work tends to focus on
particular hardware features whereas commercial work is often tied
to particular platforms.  Tuvok practically utilizes available,
modern features of GPUs while addressing and falling back to less
straightforward methods.  More importantly, it has seen significant
effort working around the nonintersecting shortcomings of modern OpenGL
drivers, a critical component of its applicability.  As such, Tuvok is
the volume renderer that is most cited outside its field.

Chapter~\ref{chp:rayguided} described ray-guided rendering and provided
a detailed performance analysis using a variety of real-world datasets.
Comparisons with previous work, including even our own
(Chapter~\ref{chp:tuvok}), demonstrate that this is the best known
method for large-scale volume visualization.  The method is capable of
providing subsecond response times with multi-terabyte datasets on
workstation hardware, as in Figure~\ref{fig:rabbit5tb}, orders of
magnitude faster than other volume rendering techniques.  The majority
of the performance increase comes from obviating the need to load large
chunks of the data from disk, based on a fine-scale identification of
early ray termination.

% Ray-guided volume rendering is the way to do volume rendering: 5 TB
% dataset in under a second!
% \begin{itemize}
% 	\item we have shown how fast such a renderer can be
% 	\item order of magnitude faster than other methods
% 	\item speedup comes mostly from only loading data that are needed
% \end{itemize}

With the ray-guided rendering solution now known, the major impediment
to high-performance volume rendering has become the data reorganization
preprocess.  Simply \emph{reading} a terabyte from a commodity disk
presently requires \tjftilde{}2.9 hours, and the bricking needed in volume
rendering consists of a great many scattered reads and writes.  At
the heart of this process is data movement: a well-known inscalable
operation.  Our dynamic rebricking technique introduced in
Chapter~\ref{chp:rayguided} ameliorates this problem, but there is no
true solution on visible on the horizon.  More work is needed in this
area.

% Reorganizing the data continues to be the bane of high-performance
% volume rendering.
% \begin{itemize}
% 	\item just reading a terabyte on a 100mb/s disk takes 2.9125 hours
% 	\item reorganization is lots of scattered writes and reads, can't perform
% 	\item at heart, data movement: this is only going to get worse
% 	\item mitigation work is possible, but still no solution on the horizon
% \end{itemize}

We have established the scalability of desktop volume rendering
solutions to be far beyond what was believed by the community.  A
single node solution is, however, unfit for extreme scale data sizes.
While many of the same techniques developed on the desktop can help,
Chapter~\ref{chp:multiscale} demonstrates that one must utilize
multi-scale parallelism, both within and across nodes.  This work
also established in the community the superiority of the `fat' node
architecture, where large numbers of cores cooperate on a single node
as opposed to a plethora of smaller nodes.  The parallelization schemes
at each scale must be considered distinctly.

%multi-scale parallelism is necessary for future scalability
%\begin{itemize}
%	\item must parallelize both across and within nodes
%	\item many of the same techniques we use on the desktop can work in dist. mem
%situations
%	\item `fat' nodes are the way to go, at scale
%	\item for volume rendering: compositing isn't a big deal; use fewer nodes
%	\item for volume rendering: load balancing continues to be a sore spot
%\end{itemize}

Chapter~\ref{chp:multiscale} examined dynamic load balancing for volume
rendering in detail.  Unfortunately these tests were inconclusive.  In
many situations dynamic load balancing resulted in similar or even
worse performance.  Load balancers also include a number of tunable
parameters that even experts have trouble setting.  More work is needed
in this area, both to ensure load balancing is beneficial and to
establish auto-tuned parameters.

% load balancing *can* help, but in general is not a good idea at scale---yet.

%	. KD-tree decomposition and binary swap compositing works well enough
%	. compositing is not an issue: don't use so many nodes.

%readback / pushing data to the GPU is really not an issue, as shown in
%both HPG2010 as well as LDAV2013.

% Mesa-based rendering is wicked slow and never worth it.

%IO remains and will remain the major problem in large-scale vis.
%\begin{itemize}
%	\item (get that Ultrascale institute graph of rendering vs. io vs. compositing)
%	-> (make point that the story would be the same if the X axis were research
%	    papers arranged by year published)
%	\item GPUs+CPUs getting very fast / HW is scaling very effectively
%	\item memory is getting smaller, so caches are less effective
%	\item HDs are getting faster (SSDs!), but not keeping pace with other elements
%	\item distributed filesystems seem locked in unscalable stream abstraction
%\end{itemize}

IO remains and will remain the major problem in large-scale
visualization.  Peterka et al. demonstrated~\cite{Peterka:2009:ETES}
that rendering and even compositing are dominated by IO at both large
and small scales.  The research community has made great strides
in parallel rendering and compositing, however perhaps a larger
contribution has come from the tremendous growth in compute and network
hardware capabilities.  Disk access has unfortunately not seen the same
improvements.

We described a number of `best practices' for IO-heavy visualization
code in Chapter~\ref{chp:io}.  One is to use large reads or writes
to maximize throughput.  If and only if this is impossible should
one turn to space-filling curve methods to minimize access times.
Despite conventional practice, the many files created by the `1 file
per process' regime scales poorly.  Most IO calls involve implicit
synchronization, and staggering these operations can have a large
impact on performance.  Along the same line, delaying file closures
until necessary can provide filesystems---especially distributed
filesystems---the time needed to do effective buffering.

% We have described a number of policies for IO code to perform well
% \begin{itemize}
% 	\item use large reads or complicated SFCurves to access `nearby' data
% 	\item avoid large numbers of smaller files
% 	\item stagger implicit IO synchronization points, such as open and close,
% 	across all processes in the parallel job
% 	\item delay file closure as long as possible
% \end{itemize}

%current IO APIs are ill-suited to the task: there is no way to specify
%information that the IO subsystem needs for efficient operation.
%	. need to match IO of distributed file storage to node requesting/using it
%	. current APIs force everything into stream interface
%	. middleware helps, but this needs to interact with VFS, at the core.

\textit{In situ} visualization is playing an increasing role
in simulation-based sciences.  The widening disparity between
memory and disk speed necessitates the approach at the largest
scales.  Furthermore, the increasingly multidisciplinary analyses
performed on simulation runs encourages analysts at many different
sites to participate, but the data are too large to be transferred
between sites.  Using knowledge of the sampling rate required by
the analysis method may be the best way of performing otherwise
computationally-infeasible analyses.  All of these factors suggest a
centralization of large-scale computing resources, in contrast to the
decentralization that industry has experienced.  It is not difficult to
imagine a future where analysis
\emph{is} simulation: that all analysis is performed by (re)running
simulations, as opposed to (re)processing simulation outputs.

%In situ visualization is playing and will play an increasingly important role
%in high-performance visualization.
%\begin{itemize}
%	\item movement of data from disk to memory is too expensive
%	\item extreme scales preclude other approaches
%	\item many analysts, and data cannot move between sites
%	\item $\leadsto$ centralization of computational resources?
%	\item sampling of analysis is the best---only?---way to limit data sizes
%\end{itemize}
%In the future, we can expect that little to no analysis will be
%performed on data from a previous run: new analysis will imply a new
%simulation run.

\textit{In situ} visualization is currently complex and difficult.
Bespoke in situ visualization solutions abound, but there are presently
only a
couple visualization and analysis tools with \insitu{} APIs that
cater to a wide variety of data models.  Stability is a problem with
these large all-encompassing tools.  This is in addition to the many
difficult
issues inherent to \insitu{} visualization, such as balancing
simulation time and visualization time.

Despite the growing importance of \insitu{}, existing visualization
tools couple with simulation software only with extreme difficulty.
This heavy
investment in tool coupling discourages \textit{ad hoc} and exploratory
solutions.  Authors currently acquiesce this effort only under extreme
pressure, which today means that data sizes and related IO performance
exceed a high threshold.  As the IO performance disparity widens, this
threshold lowers.  Eventually, the HPC community will need to consider
\insitu{} visualization for \emph{all} simulation software.

Adding a new capability (even if it is the same capability) across such
a large base of installed software is a daunting task.  Much of the
difficulty comes from the visualization software: large APIs to learn,
relatively instable software, and complex methodologies for conveying
metadata.  These sharp edges can be filed down through (currently
rather large) investments in learning the visualization tools of the
day.  However, other concerns are simulation-specific as opposed to
visualization-tool-specific.  These include identifying \emph{where}
to insert calls into the visualization tool, and striking the correct
balance between simulation time and visualization/analysis time.

%  Modern tools are suitable for the extreme end of large-scale
%simulations, but the heavy investment in tool coupling discourages
%\textit{ad hoc} and exploratory solutions.  Authors of many simulations
%run acquiesce this cost simply because traditional analysis pipelines
%are infeasible at the desired scales.

%We have shown that in situ visualization can be considerably easier
%than previously expected.
%\begin{itemize}
%	\item works with normal compiled versions
%	\item majority of metadata is worthless
%\end{itemize}

Our work addresses these complexity issues head on.
Chapters~\ref{chp:freeprocessing} and~\ref{chp:inference} outline
methods that one can use to immediately solve the issues of
injecting---at will---visualization and analysis capabilities into an
entire corpus of simulation software.  Chapter~\ref{chp:inference}
takes the first steps at eliminating the complex metadata communication
primitives.  However, we must also ameloriate the untenable model of
simulation authors becoming visualization experts.  The approaches of
both chapters put more onus on the efforts of visualization scientists,
sweeping many visualization details under the rug from the simulation
author's perspective.  An important theme among these approaches is
shifting responsibilities away from the simulation engineer and towards
the visualization engineer.
