%We are generating more data than we could possibly examine.
%	* growth is huge
%	* cite fig
%	* overwhelming ability to analyze/understand

The growth in data-based science has exploded in recent years.  As
Figure~\ref{fig:growth} shows, the size of the available data to
analyze increases exponentially.  While the increased fidelity
enables new levels of understanding, this is only possible if we can
interrogate and analyze the increased scale of data.

\begin{figure}
	\includegraphics[width=\linewidth]{images/growth}

  \caption{The growth of data in recent years.  Hardware capabilities
  have not kept pace with the exponential pace of data growth.}

	\label{fig:growth}
\end{figure}

Humans are effective pattern recognizers, but error-prone with
repetitive tasks and inherently visual thinkers.  Among even small sets
of data, it is much easier for us to apprehend a change in color than
it is a digit in the third decimal place of a repeated sequences of
numbers.  Mapping raw numbers to color or another visual representation
is thus an effective way to take advantage of the pattern recognition
capabilities of the human visual system.  The growth of data only
increases the need for data visualization, else we will be left with
massive piles of numbers with no insight into the mechanisms or
processes that sourced them.

There are a variety of data types we might apply our efforts to.  In
this thesis, we concentrate on \textit{regular gridded data}, as might
be output by medical (CT, MRI) scanners or used in simulation software.
These kinds of strutured grids account for a disproportionately large
subset of data types used in the scientific and medical domains.  It is
one of the `important' data types highlighted by a report discussing
the future scalability of computing in
science~\cite{Berkeley:2006:View}.  Furthermore, operations on such
data are easily mappable to promising data-parallel hardware, unlike
many other data types, suggesting its continued future use at scale.

%The best-represented subset of scientific data is regularly gridded
%data.
%	* limit ourselves to scientific data
%	* scientific data has spatial and potentially temporal elements
%	* regularly gridded data is disproportionately represented
%	* one of the Berkeley dwarves (5. structured grids)
%	** also fits 1. dense linear algebra

%Berkeley introduces 13 dwarves.
%	 1. dense linear algebra: yes
%	 2. sparse linear alg: no
%	 3. spectral methods: no
%	 4. n-body methods: no
%	 5. structured grids: yes
%	 6. unstructured grids: no
%	 7. monte carlo: no
%	 8. computational logic: no
%	 9. graph traversal: no
%	10. dynamic programming: no
%	11. backtrace, branch+bound: no
%	12. graphical models: no
%	13. finite state machines: no

There are a number of visualization techniques that are widely
appreciated for such volumetric data.  As the techniques have been
known for years, much of the effort has been focused in mapping the
algorithms to novel architectures, identifying special cases of
interest, and reducing constants in the order of the algorithm's
execution.

%Extreme data sizes as well as established algorithms (in SciVis)
%mean there is increasing focus on constants in algorithmic scaling
%equations.
%	* theres exists a small set of `established' vis algorithms
%	* already well-utilized for understanding structured data
%	* known performance/scaling properties
%	* recent/future research focused on reducing constants

% * vis is important
%
% * performance is critical
%
% * prevalence of regular grids

\section{Volume visualization}

%* volume rendering
%	* definition
%	* why is it used
%	* imagevis3d (not tuvok!)

Direct volume rendering produces visualizations that accurately model
tenuous mediums such as fires, smoke, and clouds.  However, it is
seldom applied for its ability to produce photorealistic imagery.  The
scientific and medical domains use volume rendering to see
\emph{inside} a data set and highlight its internal structure.
This can be extremely useful both in interrogating data as well as
communicating known features.

%Volume rendering allows us to see inside data sets.
%	* physically defined for tenuous mediums: fire, smoke, clouds
%	* usually applied elsewhere: CT, MRI, simulation data
%	* can filter out unwanted features
%	* can highlight features of interest

%Volume visualization is a useful technique for understanding the
%structure of 3D data.

The precision instrument used to control volume rendering is called the
\emph{transfer function}.  A transfer function maps the value at every
datum to colors \emph{and} opacities.  The control of opacity gives
users an effective mechanism for filtering out data irrelevant to the
point of interest.  Color control enables the user to specify the
mapping stage of the visualization pipeline.  In contrast to other
volume visualization methods, this gives comparatively more power and
flexibility to the user to communicate a feature exactly as intended.

Unfortunately this increased flexibility comes at the cost of
considerable computational complexity as compared to other volume
visualization techniques.  In the general case, volume rendering is an
$O(n^3)$ algorithm, requiring the consideration of every voxel in a
three-dimensional dataset.  Each of these operations typically consists
of trilinear interpolation coupled with a set of multiply-adds needed
to compute the volume rendering integral.  For data of even modest
size, a na\"ive CPU-based computation will be far from interactive.

%A \emph{transfer function} gives user control over the filtering and
%mapping processes of the visualization pipeline.
%	* tfqn assigns colors and opacities to data values
%	* user control of filtering vis stage
%	* user control of mapping vis stage
%	* thus, precise control over the display

%Volume rendering is computationally complex.
%	* trilinear interpolation
%	* compositing
%	* in general, $O(n^3)$

\section{Systems opportunities}

There are a number of systems-oriented challenges in the ameloriation of
algorithmic constants in volume visualization.

\subsection{Hardware \& programmability}

The end of Moore's law necessitates a reorganization in software architecture
to take advantage of novel architectures.
	* single processor performance increases have peaked
	* multiprocessor is the new norm
	* computation is free, communication costs
	* limited per-core memory available

In part due to the results of this thesis, architecting for
accelerators as opposed to CPU threads holds greater promise for
long-term performance scalability.
	* both leading companies (NVIDIA + intel) have invested in accelerators
	* accelerator model seems to be here to stay
	* performance per watt vastly improved

The exact characteristics of accelerators is a current topic of
industry competition, but the general characteristics are large numbers
of low-power cores connected to limited but high-bandwidth memory.
	* memory is where things get expensive
	* more powerful cores need more power + can't dissipate heat
	* but more cores are viable

The programmability of future high-performance systems is a competitive
topic that is presently conflated with that of the hardware.
	* many library choices
	** CUDA, OpenCL, Intel TBB, OpenMP, OpenAcc, Cilk, UPC
	* hardware choices
	** CPU threads (dead), GPUs, Xeon Phi

%	* GPUs
%	* versus CPU threads
%	* versus Phi?
%	* future architecture of supercomputers defined by current research
%	* programmability
%	* CUDA, OpenCL, OpenMP, OpenAcc

\subsection{I/O}

The storage hierarchy is the single most limiting architectural
component.
	* slowest component by far
	* gap between disk and memory is the widest HW gulf
	* huge problem for vis: synchronous!
	** vis is unique: no help from general HPC community

Parallel storage scalability is not a simple as adding more disks.
	* data movement is too expensive
	** must access data intelligently
	** false sharing
	* metadata access (storage) is a limiting component
	* tremendous contention at cluster scales

%* parallel io
%	* filesystems
%	* lustre
%	* MDSs, ODSs or whatever they're called
%	* DDoS metadata
%	* false sharing

There are no imminent advances on the horizon for the IO problems
plaguing modern visualization and analysis software.
	* SSDs help, but not nearly enough
	* only going to get more disks

\subsection{\textit{In situ} visualization}

In situ visualization addresses the `too big to read' problem in
visualization.
	* beyond a certain size, it no longer makes sense to read data
  * waiting to read large data progression: sip your coffee, get a
  coffee, make a pot, drink a cup, teach a class, sleep, forget it.
	* in situ solves this by obviating the `read' step

In situ visualization exposes difficulties in coupling visualization
and simulation codes.
	* typically written by different groups,
	* in different languages,
	* with different data models.
	* visualization software is LARGE and complex

Considerable effort has been focused on scalability and performance.
	* often serves to make these more complex
	* cite considerable work in this area

The complexity of modern \textit{in situ} solutions is crippling.
	* forgotten use case simple code that just wants to do vis
	* spend more time doing in situ than writing the simulation code
	* we seem to have forgetten that visualization is a tool to others

%* in situ visualization
%	* solves
%		* data too large to be read
%		* end-to-end 'time-to-insight' performance
%	* problems
%		* how metadata is transferred
%		* vis cannot slow down sim (much)
%		* data access from sim -> vis
%		* difficulty in coupling sim+vis
%		* how often do we update vis
%		* \emph{when} do we update vis
